<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CLIP: Learning Transferable Visual Models From Natural Language Supervision</title>
    <meta name="description" content="My detailed analysis of CLIP (Radford et al., 2021): objectives, training setup, zero-shot transfer, strengths, limitations, and what I think matters next.">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/x-icon" href="../favicon.ico">
    <link rel="apple-touch-icon" href="../favicon.svg">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="stylesheet" href="../assets/css/blog-styles.css">
    <link rel="stylesheet" href="../assets/css/clip-analysis.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <!-- MathJax for LaTeX -->
    <script>
      window.MathJax = {
        tex: {inlineMath: [["$","$"],["\\(","\\)"]], displayMath: [["$$","$$"],["\\[","\\]"]]},
        options: {skipHtmlTags: ['script','noscript','style','textarea','pre','code']}
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
</head>
<body>
    <!-- Scroll Progress Bar -->
    <div class="scroll-progress">
        <div class="scroll-progress-bar"></div>
    </div>

    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="../index.html">Khang Vo</a>
            </div>
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../index.html#about">About</a></li>
                <li><a href="../index.html#research">Research</a></li>
                <li><a href="../index.html#publications">Publications</a></li>
                <li><a href="../index.html#projects">Projects</a></li>
                <li><a href="../index.html#educational">Educational</a></li>
                <li><a href="../index.html#media">Media</a></li>
                <li><a href="blog.html" class="active">Blog</a></li>
                <li><a href="../index.html#contact">Contact</a></li>
            </ul>
            <div class="nav-controls">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle dark mode" title="Toggle dark mode">
                    <i class="fas fa-moon"></i>
                </button>
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
    </nav>

    <!-- Blog Post Header -->
    <section class="blog-post-header">
        <div class="container">
            <div class="post-breadcrumb">
                <a href="blog.html">Blog</a>
                <i class="fas fa-chevron-right"></i>
                <span>Technical</span>
            </div>

            <div class="post-header-content">
                <div class="post-category">Technical</div>
                <h1 class="post-title">CLIP: Learning Transferable Visual Models From Natural Language Supervision</h1>
                <p class="post-subtitle">Zero-shot learning, contrastive objectives, and why this paper still matters</p>

                <div class="post-meta">
                    <div class="post-author">
                        <img src="../assets/img/images/avatar.jpg" alt="Vo Hoang Nhat Khang" class="author-avatar">
                        <div class="author-info">
                            <span class="author-name">Vo Hoang Nhat Khang (Chris)</span>
                            <span class="author-title">PhD Student in NLP at MBZUAI</span>
                        </div>
                    </div>
                    <div class="post-details">
                        <span class="post-date">
                            <i class="fas fa-calendar"></i>
                            <script>
                                const date = new Date();
                                const month = date.getMonth() + 1;
                                const day = date.getDate();
                                const year = date.getFullYear();
                                document.write(`${month}/${day}/${year}`);
                            </script>
                        </span>
                        <span class="post-read-time">
                            <i class="fas fa-clock"></i>
                            12 min read
                        </span>
                    </div>
                </div>

                <div class="post-tags">
                    <span class="tag">CLIP</span>
                    <span class="tag">Contrastive Learning</span>
                    <span class="tag">Zero-shot</span>
                    <span class="tag">Multimodal</span>
                </div>
            </div>
        </div>
    </section>

    <!-- Blog Post Content -->
    <section class="blog-post-content">
        <div class="container">
            <div class="post-layout">
                <article class="post-article">
                    

                    <div class="post-body">
                        <div class="tldr-block">
                            <div class="tldr-header">
                                <i class="fas fa-lightbulb"></i>
                                <h3>TL;DR</h3>
                            </div>
                            <ul>
                                <li>CLIP trains image and text encoders jointly using contrastive learning</li>
                                <li>Enables zero-shot classification by comparing images to text prompts</li>
                                <li>Learns rich visual representations from natural language supervision</li>
                                <li>Foundation for many multimodal models and zero-shot vision applications</li>
                            </ul>
                        </div>

                        <h2 id="overview">The Big Picture</h2>
                        <p>
                            Open your photo library and try describing a picture out loud: "a sleepy orange cat on a sunlit windowsill". That's closer to how we think than a rigid label like "cat" or "dog". <a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">CLIP</a> (Radford et al., 2021) leans into that simple truth. Instead of memorizing category lists, it learns the language of images - the messy, specific, wonderfully human way we talk about what we see.
                        </p>
                        <p>
                            Under the hood, the idea is disarmingly direct: learn two encoders - one for images, one for text - and pull matching pairs together in a shared space while pushing everything else apart. Once that space exists, zero-shot classification becomes a conversation: describe what you want in natural language, compare it to the image, and pick what fits. No task-specific retraining. No bespoke datasets. Just words and pictures meeting in the middle.
                        </p>

                        <h2 id="objective">How CLIP Actually Learns</h2>
                        <p>
                            The magic happens through something called <em>contrastive learning</em>. Think of it like teaching a child to match pictures with descriptions. You show them a photo of a cat with the text "a cat sitting on a chair," and you want them to learn that this image and this text go together. But you also show them the same image with descriptions of other things like "a dog running" or "a car parked outside," and they learn that these don't match.
                        </p>
                        <p>
                            Mathematically, CLIP does this with a symmetric contrastive loss. For a batch of \(N\) image-text pairs, it computes similarities between all possible combinations and tries to maximize the similarity between matching pairs while minimizing everything else. The beauty is in the symmetry – it learns both directions: given an image, find the right text, and given text, find the right image.
                        </p>
                        \[
                            \mathcal{L} = \frac{1}{2}\Bigg( \underbrace{\frac{1}{N} \sum_{i=1}^{N} \! -\log \frac{\exp(s_{ii})}{\sum_{j=1}^{N} \exp(s_{ij})}}_{\text{image} \to \text{text}} \; + \; \underbrace{\frac{1}{N} \sum_{i=1}^{N} \! -\log \frac{\exp(s_{ii})}{\sum_{j=1}^{N} \exp(s_{ji})}}_{\text{text} \to \text{image}} \Bigg).
                        \]
                        <p>
                            This symmetric approach is what makes CLIP so robust. It's not just learning to recognize images – it's learning a true understanding of the relationship between visual and textual concepts. When I first saw this, I realized it was solving a much deeper problem than just image classification.
                        </p>

                        <h2 id="architecture">The Two-Encoder Setup</h2>
                        <p>
                            CLIP's architecture is refreshingly straightforward – no fancy tricks, just two encoders doing their job. The image encoder (usually a ResNet or Vision Transformer) takes in images and spits out embeddings. The text encoder (a Transformer) does the same for text. Both outputs get projected into the same shared space where the magic happens.
                        </p>
                        <p>
                            What I find elegant about this design is how it mirrors how we actually process information. When you see a sunset, your visual cortex processes the image while your language centers can simultaneously generate a description. CLIP essentially replicates this dual processing in a neural network.
                        </p>
                        <figure class="figure-wide">
                            <img class="figure-img" src="../assets/img/images/blog1-clip/clip-archiecture.png" alt="CLIP architecture diagram">
                            <figcaption class="figure-caption">
                                Figure: CLIP dual-encoder architecture (image and text encoders into a shared embedding space with contrastive training). Source: my annotated snapshot of the paper.
                            </figcaption>
                        </figure>

                        <h2 id="training">The Scale That Made It Work</h2>
                        <p>
                            Here's where things get interesting. CLIP was trained on a staggering $400$ million image-text pairs scraped from the web. That's not just a lot of data - it's a completely different kind of data than what we'd been using before. Instead of carefully curated datasets with perfect labels, CLIP learned from the messy, noisy, but incredibly diverse world of internet images and their captions.
                        </p>
                        <p>
                            I remember thinking when this paper came out: "Of course it works - they're using the entire internet as their training set!". But that's actually the genius of it. By training on such diverse, real-world data, CLIP learned to understand concepts that would never appear in traditional datasets. It learned about memes, artistic styles, cultural references, and all the weird, wonderful ways humans actually describe what they see.
                        </p>

                        <h2 id="zero-shot">The Magic of Zero-Shot Learning</h2>
                        <p>
                            Once you have your trained model, you can classify images into completely new categories without any additional training. Want to find images of "a dog wearing sunglasses" or "a person riding a unicycle"? Just describe it in natural language, and CLIP will find it.
                        </p>
                        <p>
                            The process is beautifully simple: you create text prompts describing what you're looking for (like "a photo of a cat" or "a person playing guitar"), encode both the image and the text prompts, then find which text embedding is most similar to the image. The math is just cosine similarity:
                        </p>
                        \[
                            \hat{k} = \arg\max_k \; \frac{\langle v, t_k \rangle}{\lVert v \rVert \, \lVert t_k \rVert}.
                        \]
                        <p>
                            What's remarkable is how well this works. I've used CLIP to search for things like "a person doing yoga on a beach at sunset" and it actually finds relevant images. It's like having a conversation with your computer about what you want to see, and it understands you perfectly.
                        </p>

                        <h2 id="results">What Blew My Mind</h2>
                        <p>
                            The results were genuinely surprising. CLIP achieved zero-shot performance that was competitive with models that had been specifically trained on those datasets. But what really got me excited were the things that traditional models couldn't do at all.
                        </p>
                        <ul>
                            <li><strong>Broad transfer:</strong> The same model that learned from internet images could suddenly classify medical X-rays, satellite imagery, or artistic paintings without any additional training. It was like watching someone who learned English from reading novels suddenly become fluent in poetry, technical manuals, and street slang.</li>
                            <li><strong>Compositionality:</strong> You could ask for things that had never been explicitly trained on. "A red car in the rain" or "a person reading a book while standing on one leg" - CLIP understood these complex, compositional concepts because it had learned the building blocks of visual understanding.</li>
                            <li><strong>Retrieval:</strong> The bidirectional nature meant you could go both ways - find images that match text descriptions, or find text descriptions that match images.</li>
                        </ul>

                        <h2 id="results-tables">The Numbers That Matter</h2>
                        <p>
                            While the qualitative improvements were impressive, the quantitative results really drove home how significant CLIP's breakthrough was. For complete details, the original paper has all the numbers: <a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">arXiv:2103.00020</a>.
                        </p>

                        <h3>Zero-shot Image Classification</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Backbone</th>
                                    <th>Setting</th>
                                    <th>Dataset(s)</th>
                                    <th>Outcome Summary</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>ResNet / ViT variants</td>
                                    <td>Zero-shot</td>
                                    <td>ImageNet (and variants)</td>
                                    <td>Competitive with a supervised ResNet-50 without using ImageNet labels; strong transfer via prompts.</td>
                                </tr>
                                <tr>
                                    <td>ResNet / ViT variants</td>
                                    <td>Zero-shot</td>
                                    <td>30+ datasets (e.g., CIFAR-100, OxfordPets, Flowers, Food, SUN397, Caltech101)</td>
                                    <td>Consistent zero-shot gains vs. chance and competitive with supervised baselines; benefits from prompt ensembles.</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Linear Probe Transfer</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Representation</th>
                                    <th>Probe</th>
                                    <th>Dataset(s)</th>
                                    <th>Outcome Summary</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>CLIP image encoder</td>
                                    <td>Linear classifier</td>
                                    <td>ImageNet and standard transfer suites</td>
                                    <td>Strong linear separability; linear probe narrows the gap to supervised training on many datasets.</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Image–Text Retrieval</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Task</th>
                                    <th>Metric</th>
                                    <th>Dataset(s)</th>
                                    <th>Outcome Summary</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Image→Text, Text→Image</td>
                                    <td>Recall@K</td>
                                    <td>Common benchmarks (e.g., Flickr/Coco)</td>
                                    <td>Competitive retrieval without task-specific training due to shared embedding space and symmetric loss.</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Robustness and Distribution Shift</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Evaluation</th>
                                    <th>Dataset(s)</th>
                                    <th>Outcome Summary</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Zero-shot under distribution shift</td>
                                    <td>ImageNetV2, ImageNet-R, ImageNet-Sketch, etc.</td>
                                    <td>Improved robustness compared to supervised ImageNet models; language priors aid generalization.</td>
                                </tr>
                            </tbody>
                        </table>

                        <p class="note-muted">
                            Note: This section is a structured summary of the paper's reported results. For exact numeric scores, please refer to the tables and figures in the paper <a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">[link]</a>.
                        </p>

                        <h2 id="limitations">The Reality Check</h2>
                        <p>
                            As much as I love CLIP, it's not perfect. The same web data that gave it such broad understanding also brought along all the biases and quirks of human society. I've seen CLIP struggle with things that seem obvious to us – it might associate certain professions with specific genders, or have trouble with images from cultures it wasn't well exposed to during training.
                        </p>
                        <ul>
                            <li><strong>Bias and spurious cues:</strong> The web is a messy place, and CLIP learned from all of it. It's a reminder that our models are only as good as the data we feed them.</li>
                            <li><strong>Localization:</strong> CLIP sees the big picture but misses the details. Ask it to find "a person pointing at something" and it might find the person, but it won't tell you exactly where they're pointing. Sometimes you need that fine-grained spatial understanding.</li>
                            <li><strong>Domain shift:</strong> When I tried using CLIP on medical images or scientific diagrams, it often struggled. The visual language of these domains is so different from typical web images that CLIP's learned representations just didn't transfer well. It's like asking someone who learned English from social media to read a physics textbook.</li>
                        </ul>

                        <h2 id="math-notes">And One More Thing...</h2>
                        <p>
                            There's one parameter in CLIP that often gets overlooked but can make a huge difference in practice: the temperature \(\tau\). It controls how "sharp" the similarity distributions are – lower values make the model more confident in its predictions, while higher values keep things more uncertain. I've found that tuning this carefully is crucial, especially when the batch composition changes.
                        </p>
                        \[
                            p_{ij}^{(I\to T)} = \frac{\exp(\langle v_i, t_j \rangle / \tau)}{\sum_{k} \exp(\langle v_i, t_k \rangle / \tau)}\,, \quad
                            p_{ij}^{(T\to I)} = \frac{\exp(\langle t_i, v_j \rangle / \tau)}{\sum_{k} \exp(\langle t_i, v_k \rangle / \tau)}.
                        \]


        

                        <h2 id="beyond-clip">Where This Led Us</h2>
                        <p>
                            CLIP didn't just solve a technical problem – it opened up a whole new way of thinking about vision and language. It showed us that you could learn powerful visual representations from natural language supervision, and that opened the floodgates for everything that came after.
                        </p>
                        <p>
                            Today, we have models like GPT-4V that can not only understand images and text together, but can actually reason about them, answer questions, and even generate new content. But they all trace their lineage back to CLIP's core insight: that language is the natural interface for visual understanding. It's like CLIP taught us the alphabet, and now we're writing novels.
                        </p>

                        <h2 id="citation">Citation</h2>
                        <p>
                            Radford, A., Kim, J. W., Hallacy, C., et al. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv, 2021. Available at <a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener">https://arxiv.org/abs/2103.00020</a>.
                        </p>
                    </div>

                    <div class="post-footer">
                        <div class="post-share">
                            <span class="share-label">Share this article:</span>
                            <div class="share-buttons">
                                <button class="share-btn twitter" onclick="sharePost('twitter')" aria-label="Share on Twitter" title="Share on Twitter">
                                    <i class="fab fa-twitter"></i>
                                </button>
                                <button class="share-btn linkedin" onclick="sharePost('linkedin')" aria-label="Share on LinkedIn" title="Share on LinkedIn">
                                    <i class="fab fa-linkedin"></i>
                                </button>
                                <button class="share-btn facebook" onclick="sharePost('facebook')" aria-label="Share on Facebook" title="Share on Facebook">
                                    <i class="fab fa-facebook"></i>
                                </button>
                                <button class="share-btn copy" onclick="copyLink()" aria-label="Copy link" title="Copy link">
                                    <i class="fas fa-link"></i>
                                </button>
                            </div>
                        </div>
                    </div>
                </article>

                <!-- Sidebar -->
                <aside class="post-sidebar">
                    <div class="sidebar-widget toc-widget">
                        <h3 class="widget-title">Table of Contents</h3>
                        <nav class="toc-nav">
                            <ul>
                                <li><a href="#overview">The Big Picture</a></li>
                                <li><a href="#objective">How CLIP Actually Learns</a></li>
                                <li><a href="#architecture">The Two-Encoder Setup</a></li>
                                <li><a href="#training">The Scale That Made It Work</a></li>
                                <li><a href="#zero-shot">The Magic of Zero-Shot Learning</a></li>
                                <li><a href="#results">What Blew My Mind</a></li>
                                <li><a href="#results-tables">The Numbers That Matter</a></li>
                                <li><a href="#limitations">The Reality Check</a></li>
                                <li><a href="#math-notes">And One More Thing...</a></li>
                                <li><a href="#beyond-clip">Where This Led Us</a></li>
                                <li><a href="#citation">Citation</a></li>
                            </ul>
                        </nav>
                    </div>
                </aside>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <p>&copy; 2025 Vo Hoang Nhat Khang (Chris). All rights reserved.</p>
                <div class="visitor-counter">
                    <i class="fas fa-eye"></i>
                    <span>Total Visitors: <span id="visitor-count">Loading...</span></span>
                </div>
            </div>
        </div>
    </footer>

    <script src="../assets/js/script.js"></script>
    <script src="../assets/js/blog-script.js"></script>
</body>
</html>


