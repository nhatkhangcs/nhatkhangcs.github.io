<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention is All You Need: The Transformer Architecture</title>
    <meta name="description" content="My detailed analysis of the Transformer architecture (Vaswani et al., 2017): self-attention, multi-head attention, and why this paper revolutionized NLP.">
    
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="icon" type="image/x-icon" href="../favicon.ico">
    <link rel="apple-touch-icon" href="../favicon.svg">
    <link rel="stylesheet" href="../assets/css/styles.css">
    <link rel="stylesheet" href="../assets/css/blog-styles.css">
    <link rel="stylesheet" href="../assets/css/transformer-analysis.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <!-- MathJax for LaTeX -->
    <script>
      window.MathJax = {
        tex: {inlineMath: [["$","$"],["\\(","\\)"]], displayMath: [["$$","$$"],["\\[","\\]"]]},
        options: {skipHtmlTags: ['script','noscript','style','textarea','pre','code']}
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
</head>
<body>
    <!-- Scroll Progress Bar -->
    <div class="scroll-progress">
        <div class="scroll-progress-bar"></div>
    </div>

    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="../index.html">Khang Vo</a>
            </div>
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../index.html#about">About</a></li>
                <li><a href="../index.html#research">Research</a></li>
                <li><a href="../index.html#publications">Publications</a></li>
                <li><a href="../index.html#projects">Projects</a></li>
                <li><a href="../index.html#educational">Educational</a></li>
                <li><a href="../index.html#media">Media</a></li>
                <li><a href="blog.html" class="active">Blog</a></li>
                <li><a href="../index.html#contact">Contact</a></li>
            </ul>
            <div class="nav-controls">
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle dark mode" title="Toggle dark mode">
                    <i class="fas fa-moon"></i>
                </button>
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
    </nav>

    <!-- Blog Post Header -->
    <section class="blog-post-header">
        <div class="container">
            <div class="post-breadcrumb">
                <a href="blog.html">Blog</a>
                <i class="fas fa-chevron-right"></i>
                <span>Technical</span>
            </div>

            <div class="post-header-content">
                <div class="post-category">Technical</div>
                <h1 class="post-title">Attention is All You Need: The Transformer Architecture</h1>
                <p class="post-subtitle">How has this paper changed everything in NLP?</p>

                <div class="post-meta">
                    <div class="post-author">
                        <img src="../assets/img/images/avatar.jpg" alt="Vo Hoang Nhat Khang" class="author-avatar">
                        <div class="author-info">
                            <span class="author-name">Vo Hoang Nhat Khang (Chris)</span>
                            <span class="author-title">PhD Student in NLP at MBZUAI</span>
                        </div>
                    </div>
                    <div class="post-details">
                        <span class="post-date">
                            <i class="fas fa-calendar"></i>
                            <script>
                                const date = new Date();
                                const month = date.getMonth() + 1;
                                const day = date.getDate();
                                const year = date.getFullYear();
                                document.write(`${month}/${day}/${year}`);
                            </script>
                        </span>
                        <span class="post-read-time">
                            <i class="fas fa-clock"></i>
                            15 min read
                        </span>
                    </div>
                </div>

                <div class="post-tags">
                    <span class="tag">Transformer</span>
                    <span class="tag">Attention</span>
                    <span class="tag">NLP</span>
                    <span class="tag">Sequence Modeling</span>
                </div>
            </div>
        </div>
    </section>

    <!-- Blog Post Content -->
    <section class="blog-post-content">
        <div class="container">
            <div class="post-layout">
                <article class="post-article">
                    

                    <div class="post-body">
                        <div class="tldr-block">
                            <div class="tldr-header">
                                <i class="fas fa-lightbulb"></i>
                                <h3>TL;DR</h3>
                            </div>
                            <ul>
                                <li>Transformers replace RNNs/CNNs with pure attention mechanisms for sequence modeling</li>
                                <li>Self-attention allows parallel processing and captures long-range dependencies</li>
                                <li>Multi-head attention enables learning different types of relationships simultaneously</li>
                                <li>Foundation for all modern language models (BERT, GPT, T5, etc.)</li>
                            </ul>
                        </div>

                        <h2 id="overview">The Paper That Changed Everything</h2>
                        <p>
                            Some papers slip quietly into your workflow; this one kicked the door in. <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a> (Vaswani et al., 2017) didn’t iterate; it replaced an era. It rewrote how we think about sequence modeling, not by adding yet another clever recurrence, but by walking away from recurrence entirely.
                        </p>
                        <p>
                            The Transformer became the substrate for almost everything that followed: BERT, GPT, T5 - and then it jumped fences into vision, speech, and multimodal systems. 
                        

                        <h2 id="motivation">Why We Needed Something Better</h2>
                        <p>
                            Before Transformers, we were stuck with RNNs and CNNs for sequence modeling, and honestly, they were frustrating to work with. I remember spending hours waiting for models to train, dealing with vanishing gradients, and constantly hitting the limits of what these architectures could do.
                        </p>
                        <p>
                            The problems were everywhere: RNNs had to process sequences one step at a time, which meant training was painfully slow. Try to learn long-range dependencies? Good luck with those vanishing gradients. Want to parallelize training? Forget it - the sequential nature made that impossible.
                        </p>
                        <p>
                            The Transformer's breakthrough was realizing we didn't need recurrence or convolution at all. Instead of processing sequences step-by-step, what if we could look at all positions simultaneously and let the model figure out which parts are important? That's exactly what attention mechanisms do - and it changed everything.
                        </p>
                        

                        <h2 id="attention-mechanism">The Heart of the Matter: Self-Attention</h2>
                        <p>
                            Self-attention is like giving the model the ability to look at every word in a sentence simultaneously and decide which ones are most important for understanding any given word. It's like having a conversation where you can instantly reference anything that was said earlier, without having to remember it step by step.
                        </p>
                        <p>
                            For each word in the sequence, the model creates three vectors: a <b>Query</b> - $\mathbf{Q}$ (what am I looking for?), a <b>Key</b> - $\mathbf{K}$ (what do I represent?), and a <b>Value</b> - $\mathbf{V}$ (what information do I contain?). Then it computes how much attention each word should pay to every other word:
                        </p>
                        \[
                            \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
                        \]
                        <p>
                            The \(\sqrt{d_k}\) scaling factor is a neat trick that prevents the attention scores from becoming too extreme when the dimensions get large. Without it, the softmax would become too peaked, and the model would essentially ignore most of the input. It's one of those small details that makes a big difference.
                        </p>


                        <figure class="figure-narrow figure-small">
                            <img
                                src="../assets/img/images/blog4-transformer/scaled-dot.png"
                                alt="Scaled dot-product attention: softmax(QK^T / sqrt(d_k)) V"
                                class="figure-img"
                                loading="lazy"
                            >
                            <figcaption class="figure-caption">
                                Scaled dot-product attention: similarities \(\mathbf{Q}\mathbf{K}^T\) are scaled by \(\sqrt{d_k}\), normalized with softmax, then applied to \(\mathbf{V}\).
                            </figcaption>
                        </figure>

                        <p>
                            But... what exactly are the $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ vectors? They are simply linear transformations of the input embeddings:
                        </p>
                        \[
                            \mathbf{Q} = \mathbf{W}^Q \mathbf{x}
                        \]
                        \[
                            \mathbf{K} = \mathbf{W}^K \mathbf{x}
                        \]
                        \[
                            \mathbf{V} = \mathbf{W}^V \mathbf{x}
                        \]
                        <p>
                        where $\mathbf{W}^Q$, $\mathbf{W}^K$, and $\mathbf{W}^V$ are the weight matrices for the query, key, and value projections; $\mathbf{x}$ is the input embedding.
                        </p>
                        
                        <div class="qa-box">
                            <div class="qa-header">
                                <i class="fas fa-question-circle"></i>
                                <span>Quick check</span>
                            </div>
                            <p class="qa-question">Question: What are the shapes of $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$?</p>
                            <p class="qa-answer">Answer: $\mathbf{Q}$ is of shape $(n, d_k)$, $\mathbf{K}$ is of shape $(n, d_k)$, and $\mathbf{V}$ is of shape $(n, d_v)$, where $n$ is the number of tokens in the sequence, $d_k$ is the dimension of the key, and $d_v$ is the dimension of the value.</p>
                        </div>


                        <h2 id="multi-head-attention">Why One Head Isn't Enough</h2>
                        <p>
                            Instead of just one attention mechanism, Transformers use multiple "heads" running in parallel. It's like having several people read the same text simultaneously, each looking for different patterns.
                        </p>
                        <p>
                            One head might focus on grammatical relationships (like which verb goes with which subject), while another might pay attention to semantic connections (like which words are related in meaning). A third might look for positional relationships (like which words are close together). By combining all these different perspectives, the model gets a much richer understanding of the text.
                        </p>
                        \[
                            \text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
                        \]
                        <p>
                            Each head learns its own set of attention patterns, and then they all get combined at the end. It's like having a team of experts, each with their own specialty, all working together to understand the same input.
                        </p>
                        
                        <figure class="figure-narrow">
                            <img
                                src="../assets/img/images/blog4-transformer/multi-head.png"
                                alt="Multi-head attention: multiple parallel attention heads concatenated and projected"
                                class="figure-img"
                                loading="lazy"
                            >
                            <figcaption class="figure-caption">
                                Multi-head attention runs several attention heads in parallel, then concatenates and projects them to mix information from different representation subspaces.
                            </figcaption>
                        </figure>
                        

                        <h2 id="architecture">Putting It All Together</h2>
                        <p>
                            Now we get to see how all these pieces fit together. The Transformer uses an encoder-decoder architecture, but it's not like the old RNN-based ones. Both the encoder and decoder are built from the same basic building blocks: attention mechanisms, feed-forward networks, and residual connections.
                        </p>
                        <p>
                            The encoder takes your input sequence and creates rich representations that capture all the relationships between words. The decoder then uses these representations to generate the output, but here's the clever part: it can attend to the entire input sequence at once, not just the previous words. This is what makes translation and other sequence-to-sequence tasks so much more effective.
                        </p>

                        <figure class="figure-narrow">
                            <img
                                src="../assets/img/images/blog4-transformer/transformer-architecture.png"
                                alt="Transformer encoder-decoder architecture with self-attention, cross-attention, and feed-forward layers"
                                class="figure-img"
                                loading="lazy"
                            >
                            <figcaption class="figure-caption">
                                High-level Transformer architecture: stacked encoder and decoder blocks with self-attention, cross-attention, residual connections, and feed-forward networks.
                            </figcaption>
                        </figure>

                        <h3>Encoder Layer</h3>
                        <p>
                            Each encoder layer consists of:
                        </p>
                        \[
                            \mathbf{x} = \text{LayerNorm}(\mathbf{x} + \text{MultiHeadAttention}(\mathbf{x}))
                        \]
                        \[
                            \mathbf{x} = \text{LayerNorm}(\mathbf{x} + \text{FFN}(\mathbf{x}))
                        \]
                        <p>
                            Where FFN is a position-wise feed-forward network: \(\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\)
                        </p>

                        <h3>Decoder Layer</h3>
                        <p>
                            Each decoder layer has three sub-layers:
                        </p>
                        <ol>
                            <li><strong>Masked self-attention:</strong> Prevents attending to future positions</li>
                            <li><strong>Encoder-decoder attention:</strong> Attends to encoder outputs</li>
                            <li><strong>Position-wise FFN:</strong> Same as encoder</li>
                        </ol>

                        <h2 id="positional-encoding">The Position Problem</h2>
                        <p>
                            Initially, this confused me about Transformers: they don't naturally understand word order. Unlike RNNs that process words sequentially, Transformers look at all words at once, so "the cat sat on the mat" and "the mat sat on the cat" would look identical to them.
                        </p>
                        <p>
                            The solution that was proposed in the paper was to add some special numbers to each word that encode its position in the sequence. These positional encodings use sine and cosine functions with different frequencies:
                        </p>
                        \[
                            PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
                        \]
                        \[
                            PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
                        \]
                        <p>
                            The beauty of this approach is that it can handle sequences of any length, and the model can learn to understand relative positions (like "the word 3 positions before this one"). This gives each word a unique address that tells the model where it belongs in the sequence.
                        </p>
                        


                        <h2 id="training">The Training Recipe</h2>
                        <p>
                            Getting Transformers to train well required some careful tuning. The authors used several techniques that have since become standard practice:
                        </p>
                        <ul>
                            <li><strong>Optimizer:</strong> Adam with \(\beta_1 = 0.9\), \(\beta_2 = 0.98\), \(\epsilon = 10^{-9}\)</li>
                            <li><strong>Learning rate:</strong> \(lrate = d_{model}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})\)</li>
                            <li><strong>Regularization:</strong> Dropout (\(p = 0.1\)) and label smoothing (\(\epsilon_{ls} = 0.1\))</li>
                            <li><strong>Batch size:</strong> $25,000$ tokens per batch</li>
                        </ul>
                        

                        <h2 id="results">The Results</h2>
                        <p>
                            When the results came in, they were impossible to ignore. The Transformer didn't just beat the previous state-of-the-art – it crushed it, and it did so while training much faster than anything that came before.
                        </p>
                        <p>
                            On the standard machine translation benchmarks, the improvements were dramatic: 28.4 BLEU on English-German (up from 25.16) and 41.8 BLEU on English-French (up from 40.46). But what really caught my attention was the training time: just 3.5 days on 8 GPUs, compared to weeks for the previous best RNN-based models.
                        </p>
                        <p>
                            This wasn't just about better performance – it was about making the impossible possible. Suddenly, you could train large language models in days instead of weeks. You could experiment with different architectures without waiting months for results. The Transformer didn't just improve the state of the art; it democratized it.
                        </p>
                    

                        <h3>Machine Translation Results</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Model</th>
                                    <th>WMT $2014$ EN-DE BLEU</th>
                                    <th>WMT $2014$ EN-FR BLEU</th>
                                    <th>Training Time</th>
                                    <th>Parameters</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Transformer (base)</td>
                                    <td>$28.4$</td>
                                    <td>$41.8$</td>
                                    <td>$12$ hours</td>
                                    <td>$65M$</td>
                                </tr>
                                <tr>
                                    <td>Transformer (big)</td>
                                    <td>$28.9$</td>
                                    <td>$41.4$</td>
                                    <td>$3.5$ days</td>
                                    <td>$213M$</td>
                                </tr>
                                <tr>
                                    <td>ConvS2S</td>
                                    <td>$25.16$</td>
                                    <td>$40.46$</td>
                                    <td>$9$ days</td>
                                    <td>$38M$</td>
                                </tr>
                                <tr>
                                    <td>ByteNet</td>
                                    <td>$23.75$</td>
                                    <td>$-$</td>
                                    <td>$-$</td>
                                    <td>$-$</td>
                                </tr>
                                <tr>
                                    <td>GNMT</td>
                                    <td>$24.6$</td>
                                    <td>$39.92$</td>
                                    <td>$-$</td>
                                    <td>$-$</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Model Variants and Ablations</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Model Variant</th>
                                    <th>EN-DE BLEU</th>
                                    <th>Parameters</th>
                                    <th>Training Time</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Transformer ($6$ layers)</td>
                                    <td>$28.4$</td>
                                    <td>$65M$</td>
                                    <td>$12$ hours</td>
                                </tr>
                                <tr>
                                    <td>Transformer ($2$ layers)</td>
                                    <td>$27.3$</td>
                                    <td>$37M$</td>
                                    <td>$6$ hours</td>
                                </tr>
                                <tr>
                                    <td>Transformer ($1$ layer)</td>
                                    <td>$26.5$</td>
                                    <td>$28M$</td>
                                    <td>$3$ hours</td>
                                </tr>
                                <tr>
                                    <td>Single-head attention</td>
                                    <td>$27.3$</td>
                                    <td>$65M$</td>
                                    <td>$12$ hours</td>
                                </tr>
                                <tr>
                                    <td>No positional encoding</td>
                                    <td>$26.8$</td>
                                    <td>$65M$</td>
                                    <td>$12$ hours</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>Computational Complexity Comparison</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>Layer Type</th>
                                    <th>Complexity per Layer</th>
                                    <th>Sequential Operations</th>
                                    <th>Maximum Path Length</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Self-Attention</td>
                                    <td>$O(n^2d)$</td>
                                    <td>$O(1)$</td>
                                    <td>$O(1)$</td>
                                </tr>
                                <tr>
                                    <td>Recurrent</td>
                                    <td>$O(nd^2)$</td>
                                    <td>$O(n)$</td>
                                    <td>$O(n)$</td>
                                </tr>
                                <tr>
                                    <td>Convolutional</td>
                                    <td>$O(knd^2)$</td>
                                    <td>$O(1)$</td>
                                    <td>$O(\log_k(n))$</td>
                                </tr>
                                <tr>
                                    <td>Self-Attention (restricted)</td>
                                    <td>$O(rn^2d)$</td>
                                    <td>$O(1)$</td>
                                    <td>$O(n/r)$</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2 id="advantages">What Made Transformers So Powerful</h2>
                        <p>
                            The advantages of Transformers go beyond just better performance. They fundamentally changed how we think about sequence modeling:
                        </p>
                        <ul>
                            <li><strong>Parallelization:</strong> Unlike RNNs that had to process sequences step-by-step, Transformers can look at all positions simultaneously. This made training much faster and more efficient.</li>
                            <li><strong>Long-range dependencies:</strong> Any two words in a sequence can directly interact, no matter how far apart they are. This solved the vanishing gradient problem that plagued RNNs.</li>
                            <li><strong>Interpretability:</strong> The attention weights actually tell us what the model is focusing on, which is incredibly useful for understanding and debugging.</li>
                            <li><strong>Flexibility:</strong> Transformers naturally handle sequences of any length without architectural changes, making them much more versatile than their predecessors.</li>
                            <li><strong>Efficiency:</strong> For long sequences, Transformers are significantly faster to train than RNNs, even though the attention mechanism has quadratic complexity.</li>
                        </ul>
                        

                        <h2 id="limitations">The Trade-offs We Had to Accept</h2>
                        <p>
                            As powerful as Transformers are, they come with their own set of challenges that researchers are still working to solve:
                        </p>
                        <ul>
                            <li><strong>Quadratic complexity:</strong> The attention mechanism scales as $O(n^2)$ with sequence length, which becomes a real bottleneck for very long sequences. This is why we're seeing so much research into more efficient attention variants.</li>
                            <li><strong>Memory requirements:</strong> Storing attention matrices for long sequences can quickly consume all available GPU memory. I've run into this limitation more times than I'd like to admit.</li>
                            <li><strong>Position encoding:</strong> The fixed sinusoidal positional encodings work well for training lengths, but they don't always generalize to much longer sequences during inference.</li>
                            <li><strong>Inductive bias:</strong> Unlike RNNs and CNNs, Transformers have less built-in structure, which means they need more data to learn effectively. This is both a strength and a weakness.</li>
                            <li><strong>Training instability:</strong> Getting the hyperparameters just right can be tricky, and small changes can sometimes lead to training failures. It's one of those things that gets easier with experience.</li>
                        </ul>
                        

                        <h2 id="impact">The Era of Transformers</h2>
                        <p>
                            Looking back, it's hard to overstate how much the Transformer changed the field. What started as a machine translation paper became the foundation for an entire generation of AI systems:
                        </p>
                        <ul>
                            <li><strong>Foundation for modern LLMs:</strong> BERT, GPT, T5, PaLM, ChatGPT, etc.</li>
                            <li><strong>Beyond NLP:</strong> Vision Transformers, speech processing, multimodal models</li>
                            <li><strong>Research acceleration:</strong> Enabled rapid progress in AI</li>
                            <li><strong>Industry adoption:</strong> Used in virtually all production language systems</li>
                            <li><strong>New paradigms:</strong> Prompting, in-context learning, instruction following</li>
                        </ul>
                
        
                        

                        <h2 id="citation">Citation</h2>
                        <p>
                            Vaswani, A., Shazeer, N., Parmar, N., et al. "Attention is All You Need." NeurIPS 2017. Available at <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">https://arxiv.org/abs/1706.03762</a>.
                        </p>
                    </div>

                    <div class="post-footer">
                        <div class="post-share">
                            <span class="share-label">Share this article:</span>
                            <div class="share-buttons">
                                <button class="share-btn twitter" onclick="sharePost('twitter')" aria-label="Share on Twitter" title="Share on Twitter">
                                    <i class="fab fa-twitter"></i>
                                </button>
                                <button class="share-btn linkedin" onclick="sharePost('linkedin')" aria-label="Share on LinkedIn" title="Share on LinkedIn">
                                    <i class="fab fa-linkedin"></i>
                                </button>
                                <button class="share-btn facebook" onclick="sharePost('facebook')" aria-label="Share on Facebook" title="Share on Facebook">
                                    <i class="fab fa-facebook"></i>
                                </button>
                                <button class="share-btn copy" onclick="copyLink()" aria-label="Copy link" title="Copy link">
                                    <i class="fas fa-link"></i>
                                </button>
                            </div>
                        </div>
                    </div>
                </article>

                <!-- Sidebar -->
                <aside class="post-sidebar">
                    <div class="sidebar-widget toc-widget">
                        <h3 class="widget-title">Table of Contents</h3>
                        <nav class="toc-nav">
                            <ul>
                                <li><a href="#overview">The Paper That Changed Everything</a></li>
                                <li><a href="#motivation">Why We Needed Something Better</a></li>
                                <li><a href="#attention-mechanism">The Heart of the Matter: Self-Attention</a></li>
                                <li><a href="#multi-head-attention">Why One Head Isn't Enough</a></li>
                                <li><a href="#architecture">Putting It All Together</a></li>
                                <li><a href="#positional-encoding">The Position Problem</a></li>
                                <li><a href="#training">The Training Recipe</a></li>
                                <li><a href="#results">The Results</a></li>
                                <li><a href="#advantages">What Made Transformers So Powerful</a></li>
                                <li><a href="#limitations">The Trade-offs We Had to Accept</a></li>
                                <li><a href="#impact">The Era of Transformers</a></li>
                                <li><a href="#citation">Citation</a></li>
                            </ul>
                        </nav>
                    </div>
                </aside>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <p>&copy; 2025 Vo Hoang Nhat Khang (Chris). All rights reserved.</p>
                <div class="visitor-counter">
                    <i class="fas fa-eye"></i>
                    <span>Total Visitors: <span id="visitor-count">Loading...</span></span>
                </div>
            </div>
        </div>
    </footer>

    <script src="../assets/js/script.js"></script>
    <script src="../assets/js/blog-script.js"></script>
</body>
</html>
